{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention is All You Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN模型\n",
    "\n",
    "## one2many\n",
    "![avatar](../img_att/one2many.jpeg)\n",
    "\n",
    "## many2one\n",
    "![avatar](../img_att/many2one.jpeg)\n",
    "\n",
    "## many2many\n",
    "![avatar](../img_att/many2man.jpeg)\n",
    "\n",
    "## seq2seq\n",
    "![avatar](../img_att/seq2seq.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-sequence models\n",
    "是深度学习模型，在机器翻译，文本摘要和图像字幕等任务中取得了很大成功。谷歌翻译于2016年底开始在生产中使用这种模型。Sequence-to-sequence是一个模型，它采用一系列项目（单词，字母，图像的特征等）并输出另一个项目序列。 经过训练的模型可以这样工作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"920\" height=\"340\" controls>\n",
       "  <source src=\"../img_att/seq2seq_1.mp4\" type=\"video/mp4\">\n",
       "</video>\n",
       "\n",
       "<video width=\"920\" height=\"340\" controls>\n",
       "  <source src=\"../img_att/seq2seq_2.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"920\" height=\"340\" controls>\n",
    "  <source src=\"../img_att/seq2seq_1.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\n",
    "<video width=\"920\" height=\"340\" controls>\n",
    "  <source src=\"../img_att/seq2seq_2.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解开这个model看一下是什么样子的 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"920\" height=\"340\" controls>\n",
       "  <source src=\"../img_att/seq2seq_3.mp4\" type=\"video/mp4\">\n",
       "</video>\n",
       "\n",
       "\n",
       "<video width=\"920\" height=\"340\" controls>\n",
       "  <source src=\"../img_att/seq2seq_4.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"920\" height=\"340\" controls>\n",
    "  <source src=\"../img_att/seq2seq_3.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\n",
    "\n",
    "<video width=\"920\" height=\"340\" controls>\n",
    "  <source src=\"../img_att/seq2seq_4.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font size=5> 这里的context是一串向量, </font>\n",
    "![avatar](../img_att/context.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单介绍一下RNN的概念 ,RNN在每个时间步进行两个输入：输入（在编码器的情况下，输入句子中的一个单词）和隐藏状态。 然而，这个词需要用矢量来表示。 为了将单词转换为向量，我们转向称为“单词嵌入”算法的方法类。word2vec之类的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"920\" height=\"340\" controls>\n",
       "  <source src=\"./RNN_1.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"920\" height=\"340\" controls>\n",
    "  <source src=\"../img_att/RNN_1.mp4\" type=\"video/mp4\">\n",
    "</video>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**现在让我们看一下可视化序列到序列模型的另一种方法。 此动画将更容易理解描述这些模型的静态图形。 这称为“展开”视图，我们不是显示一个解码器，而是显示每个时间步的副本。 这样我们就可以查看每个时间步的输入和输出。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"920\" height=\"340\" controls>\n",
       "  <source src=\"./seq2seq_5.mp4\" type=\"video/mp4\">\n",
       "</video>\n",
       "\n",
       "\n",
       "<video width=\"920\" height=\"340\" controls>\n",
       "  <source src=\"./seq2seq_6.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"920\" height=\"340\" controls>\n",
    "  <source src=\"../img_att/seq2seq_5.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\n",
    "\n",
    "<video width=\"920\" height=\"340\" controls>\n",
    "  <source src=\"../img_att/seq2seq_6.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**其实基础的Seq2Seq是有很多弊端的，首先Encoder将输入编码为固定大小状态向量的过程实际上是一个信息“信息有损压缩”的过程，如果信息量越大，那么这个转化向量的过程对信息的损失就越大，同时，随着sequence length的增加，意味着时间维度上的序列很长，RNN模型也会出现梯度弥散。最后，基础的模型连接Encoder和Decoder模块的组件仅仅是一个固定大小的状态向量，这使得Decoder无法直接去关注到输入信息的更多细节。由于基础Seq2Seq的种种缺陷，随后引入了Attention的概念**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.编码器将更多数据传递给解码器。 编码器不是传递编码阶段的最后隐藏状态，而是将所有隐藏状态传递给解码器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"920\" height=\"340\" controls>\n",
       "  <source src=\"./seq2seq_7.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"920\" height=\"340\" controls>\n",
    "  <source src=\"../img_att/seq2seq_7.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attention在产生其输出之前执行额外步骤。 为了专注于与此解码时间步骤相关的输入部分，解码器执行以下操作：\n",
    "\n",
    "## 查看它收到的编码器隐藏状态集 - 每个编码器隐藏状态与输入句子中的某个单词最相关\n",
    "## 给每个隐藏状态一个分数（让我们忽略分数目前是如何完成的）\n",
    "## 将每个隐藏状态乘以其softmax得分，从而放大具有高分的隐藏状态，并淹没低分数的隐藏状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"920\" height=\"340\" controls>\n",
       "  <source src=\"./attention_process.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"920\" height=\"340\" controls>\n",
    "  <source src=\"../img_att/attention_process.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 该attention score在decode侧的每个time step完成。\n",
    "\n",
    "## 现在让我们在下面的可视化中将整个事物放在一起，看看注意过程是如何工作的：\n",
    "\n",
    "## Attention decode RNN接收end令牌的embedding和初始解码器隐藏状态。\n",
    "## RNN处理其输入，产生输出和新的隐藏状态向量（h4）。 输出被丢弃。\n",
    "## Attention：我们使用编码器隐藏状态和h4向量来计算该时间步长的上下文向量（C4）。\n",
    "## 我们将h4和C4连接成一个向量。\n",
    "## 我们通过前馈神经网络（与模型共同训练的一个）传递此向量。\n",
    "## 前馈神经网络的输出指示该时间步长的输出。\n",
    "## 重复下一步的步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"920\" height=\"340\" controls>\n",
       "  <source src=\"../img_att/attention_tensor_dance.mp4\" type=\"video/mp4\">\n",
       "</video>\n",
       "\n",
       "<video width=\"920\" height=\"340\" controls>\n",
       "  <source src=\"../img_att/seq2seq_9.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"920\" height=\"340\" controls>\n",
    "  <source src=\"../img_att/attention_tensor_dance.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\n",
    "<video width=\"920\" height=\"340\" controls>\n",
    "  <source src=\"../img_att/seq2seq_9.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
