{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推荐算法代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 关联规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './machinelearninginaction/Ch12/kosarak.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-26854058c07f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0mparse_dat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./machinelearninginaction/Ch12/kosarak.dat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     \u001b[0minit_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_init_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_dat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0mmy_fp_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_header_tab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './machinelearninginaction/Ch12/kosarak.dat'"
     ]
    }
   ],
   "source": [
    "#FP-growth挖掘频繁项集\n",
    "from numpy import *\n",
    "from time import *\n",
    "\n",
    "def load_simple_data():\n",
    "    \"\"\"\n",
    "    Function:\n",
    "        创建加载简单数据集\n",
    "    Parameters:\n",
    "        无\n",
    "    Returns:\n",
    "         simple_data - 简单数据集\n",
    "    Modify:\n",
    "        2018-12-27\n",
    "    \"\"\"\n",
    "    simple_data = [['r', 'z', 'h', 'j', 'p'],\n",
    "                   ['z', 'y', 'x', 'w', 'v', 'u', 't', 's'],\n",
    "                   ['z'],\n",
    "                   ['r', 'x', 'n', 'o', 's'],\n",
    "                   ['y', 'r', 'x', 'z', 'q', 't', 'p'],\n",
    "                   ['y', 'z', 'x', 'e', 'q', 's', 't', 'm']]\n",
    "    return simple_data\n",
    "\n",
    "\n",
    "def create_init_set(data_set):\n",
    "    \"\"\"\n",
    "    Function:\n",
    "        计算项集在数据集的出现次数\n",
    "    Parameters:\n",
    "        data_set - 数据集\n",
    "    Returns:\n",
    "         ret_dict - 包含出现次数的项集字典\n",
    "    Modify:\n",
    "        2018-12-27\n",
    "    \"\"\"\n",
    "    ret_dict = {}\n",
    "    for trans in data_set:\n",
    "        ret_dict[frozenset(trans)] = 1\n",
    "    return ret_dict\n",
    "\n",
    "\n",
    "def update_tree(items, in_tree, header_table, count):\n",
    "    \"\"\"\n",
    "    Function:\n",
    "        更新树节点，让FP树生长\n",
    "    Parameters:\n",
    "        items - 项集\n",
    "        in_tree - 当前FP树\n",
    "        header_table - 头指针表\n",
    "        count - 次数\n",
    "    Returns:\n",
    "         无\n",
    "    Modify:\n",
    "        2018-12-27\n",
    "    \"\"\"\n",
    "    # 判断排序后列表的第一个元素是否已经是根节点的子节点\n",
    "    if items[0] in in_tree.children:\n",
    "        # 添加出现次数\n",
    "        # children = {}\n",
    "        in_tree.children[items[0]].inc(count)\n",
    "    else:\n",
    "        # 创建根节点的子节点\n",
    "        in_tree.children[items[0]] = TreeNode(items[0], count, in_tree)\n",
    "        # 如果该元素的后继节点不存在，则直接加入。如果有后继节点，则遍历链表尾部将其加入\n",
    "        if header_table[items[0]][1] == None:\n",
    "            header_table[items[0]][1] = in_tree.children[items[0]]\n",
    "        else:\n",
    "            update_header(header_table[items[0]][1], in_tree.children[items[0]])\n",
    "    # 列表元素长度大于1，递归调用更新FP树函数\n",
    "    if len(items) > 1:\n",
    "        update_tree(items[1::], in_tree.children[items[0]], header_table, count)\n",
    "\n",
    "\n",
    "def update_header(node_to_test, target_node):\n",
    "    \"\"\"\n",
    "    Function:\n",
    "        更新头指针表的节点链接\n",
    "    Parameters:\n",
    "        node_to_test - 遍历节点\n",
    "        target_node - 目标节点\n",
    "    Returns:\n",
    "         无\n",
    "    Modify:\n",
    "        2018-12-27\n",
    "    \"\"\"\n",
    "    # 遍历到链表尾节点\n",
    "    while (node_to_test.node_link != None):\n",
    "        node_to_test = node_to_test.node_link\n",
    "    # 将刚添加的树节点加入链表的尾部\n",
    "    node_to_test.node_link = target_node\n",
    "\n",
    "\n",
    "def create_tree(data_set, min_sup=1):\n",
    "    \"\"\"\n",
    "    Function:\n",
    "        遍历数据集两次构建FP树\n",
    "    Parameters:\n",
    "        data_set - 包含项集出现次数的数据集字典\n",
    "        min_sup - 最小支持度\n",
    "    Returns:\n",
    "         ret_tree - FP树\n",
    "         hearder_table - 头指针表\n",
    "    Modify:\n",
    "        2018-12-27\n",
    "    \"\"\"\n",
    "    hearder_table = {}\n",
    "    # 第一次遍历数据集，获取单个元素的次数\n",
    "    for trans in data_set:\n",
    "        for item in trans:\n",
    "            hearder_table[item] = hearder_table.get(item, 0) + data_set[trans]\n",
    "    # 去除不满足最小支持度的单个元素\n",
    "    for k in list(hearder_table.keys()):\n",
    "        if hearder_table[k] < min_sup:\n",
    "            del (hearder_table[k])\n",
    "    freq_item_set = set(hearder_table.keys())\n",
    "    # 无频繁项就直接返回\n",
    "    if len(freq_item_set) == 0:\n",
    "        return None, None\n",
    "    # 扩展头指针表，添加指向每种类型第一个元素的指针（节点链接）\n",
    "    for k in hearder_table:\n",
    "        hearder_table[k] = [hearder_table[k], None]\n",
    "    # 创建根节点\n",
    "    ret_tree = TreeNode('Null Set', 1, None)\n",
    "    # 第二次遍历数据集，构建FP树\n",
    "    for tran_set, count in data_set.items():\n",
    "        # tran_set: frozenset({'h', 'p', 'z', 'j', 'r'})\n",
    "        # count: 1\n",
    "        local_d = {}\n",
    "        # 如果单个元素是频繁项，则加入localD列表\n",
    "        for item in tran_set:\n",
    "            if item in freq_item_set:\n",
    "                # hearder_table:{'b': [3, None]}\n",
    "                local_d[item] = hearder_table[item][0]\n",
    "        # localD: {'r': 3, 'j': 1, 'z': 5, 'h': 1, 'p': 2}\n",
    "        if len(local_d) > 0:\n",
    "            # 元素按出现次数排序\n",
    "            ordered_items = [v[0] for v in sorted(local_d.items(), key=lambda p: p[1], reverse=True)]\n",
    "            # 更新FP树，让FP树生长\n",
    "            update_tree(ordered_items, ret_tree, hearder_table, count)\n",
    "    return ret_tree, hearder_table\n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "    # name：节点元素名称，在构造时初始化为给定值\n",
    "    # count：出现次数，在构造时初始化为给定值\n",
    "    # node_link：指向下一个相似节点的指针，默认为None\n",
    "    # parent：指向父节点的指针，在构造时初始化为给定值\n",
    "    # children：指向子节点的字典，以子节点的元素名称为键，指向子节点的指针为值，初始化为空字典\n",
    "    def __init__(self, name_value, num_occur, parent_node):\n",
    "        self.name = name_value\n",
    "        self.count = num_occur\n",
    "        self.node_link = None\n",
    "        self.parent = parent_node\n",
    "        self.children = {}\n",
    "\n",
    "    def inc(self, num_occur):\n",
    "        # 增加节点出现次数\n",
    "        self.count += num_occur\n",
    "\n",
    "    def disp(self, ind=1):\n",
    "        # 用于将树以文本形式显示\n",
    "        print('  ' * ind, self.name, ' ', self.count)\n",
    "        for child in self.children.values():\n",
    "            child.disp(ind + 1)\n",
    "\n",
    "\n",
    "def ascend_tree(leaf_node, prefix_path):\n",
    "    \"\"\"\n",
    "    Function:\n",
    "        根据当前节点向前追溯至根节点，记录前缀路径\n",
    "    Parameters:\n",
    "        leaf_node - 给定元素项节点\n",
    "        prefix_path - 前缀路径列表\n",
    "    Returns:\n",
    "         无\n",
    "    Modify:\n",
    "        2019-1-6\n",
    "    \"\"\"\n",
    "    # 如果节点有父节点，则将当前节点添加至前缀路径中，之后再递归向上追溯\n",
    "    if leaf_node.parent != None:\n",
    "        prefix_path.append(leaf_node.name)\n",
    "        ascend_tree(leaf_node.parent, prefix_path)\n",
    "\n",
    "\n",
    "def find_prefix_path(base_pat, tree_node):\n",
    "    \"\"\"\n",
    "    Function:\n",
    "        发现以给定元素项结尾的所有前缀路径\n",
    "    Parameters:\n",
    "        base_pat - 元素项\n",
    "        tree_node - 需遍历节点\n",
    "    Returns:\n",
    "         cond_pats - 所有条件模式基字典\n",
    "    Modify:\n",
    "        2019-1-6\n",
    "    \"\"\"\n",
    "    # 所有条件模式基字典\n",
    "    cond_pats = {}\n",
    "    # 遍历该节点的整个链表节点，记录每个节点的前缀路径，并将其添加至条件模式基当中\n",
    "    while tree_node != None:\n",
    "        prefix_path = []\n",
    "        ascend_tree(tree_node, prefix_path)\n",
    "        # 因为该节点也被加进了路径当中，所以需要路径的长度大于1\n",
    "        if len(prefix_path) > 1:\n",
    "            # 如果有前缀路径，则将前缀路径加入条件模式基集合中，并且将该元素在该前缀路径中出现的次数也添加进去\n",
    "            cond_pats[frozenset(prefix_path[1:])] = tree_node.count\n",
    "        # 当前节点的条件模式基查找完毕后，继续查找头指针链表中下一个节点的条件模式基\n",
    "        tree_node = tree_node.node_link\n",
    "    return cond_pats\n",
    "\n",
    "\n",
    "def mine_tree(in_tree, header_table, min_sup, pre_fix, freq_item_list):\n",
    "    \"\"\"\n",
    "    Function:\n",
    "        创建条件模式树\n",
    "    Parameters:\n",
    "        in_tree - FP树\n",
    "        header_table - 头指针表\n",
    "        min_sup - 最小支持度\n",
    "        pre_fix - 上一次递归的频繁项集合的前缀\n",
    "        freq_item_list - 频繁项集列表\n",
    "    Returns:\n",
    "        无\n",
    "    Modify:\n",
    "        2019-1-6\n",
    "    \"\"\"\n",
    "    # 对头指针表中的元素项按照其出现频率从小到大进行排序\n",
    "    big_l = [v[0] for v in sorted(header_table.items(), key=lambda p:p[1][0])]\n",
    "    # 遍历单元素频繁集\n",
    "    for base_pat in big_l:\n",
    "        new_freq_set = pre_fix.copy()\n",
    "        new_freq_set.add(base_pat)\n",
    "        freq_item_list.append(new_freq_set)\n",
    "        # 获得该元素的所有条件模式基，相当于一个事务集合\n",
    "        cond_patt_bases = find_prefix_path(base_pat, header_table[base_pat][1])\n",
    "        # 根据所有条件模式基集合来构建条件模式树\n",
    "        my_cond_tree, my_head = create_tree(cond_patt_bases, min_sup)\n",
    "        # 如果条件模式树的头指针表不空(每次建树时对元素支持度有要求\n",
    "        # 如果小于支持度则该元素不参与建树过程，所以在建树时，条件模式基中的元素会越来越少，最后会是空树)，则递归建树\n",
    "        if my_head != None:\n",
    "            print('conditional tree for:', new_freq_set)\n",
    "            my_cond_tree.disp(1)\n",
    "            mine_tree(my_cond_tree, my_head, min_sup, new_freq_set, freq_item_list)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # simple_data = load_simple_data()\n",
    "    # init_data = create_init_set(simple_data)\n",
    "    # print(simple_data)\n",
    "    # print(init_data)\n",
    "\n",
    "    # simple_data = load_simple_data()\n",
    "    # init_data = create_init_set(simple_data)\n",
    "    # my_fp_tree, my_header_tab = create_tree(init_data, 3)\n",
    "    # my_fp_tree.disp()\n",
    "\n",
    "    # simple_data = load_simple_data()\n",
    "    # init_data = create_init_set(simple_data)\n",
    "    # my_fp_tree, my_header_tab = create_tree(init_data, 3)\n",
    "    # cond_pats_1 = find_prefix_path('x', my_header_tab['x'][1])\n",
    "    # print(cond_pats_1)\n",
    "    # cond_pats_2 = find_prefix_path('z', my_header_tab['z'][1])\n",
    "    # print(cond_pats_2)\n",
    "    # cond_pats_3 = find_prefix_path('r', my_header_tab['r'][1])\n",
    "    # print(cond_pats_3)\n",
    "\n",
    "    # simple_data = load_simple_data()\n",
    "    # init_data = create_init_set(simple_data)\n",
    "    # my_fp_tree, my_header_tab = create_tree(init_data, 3)\n",
    "    # freq_items = []\n",
    "    # mine_tree(my_fp_tree, my_header_tab, 3, set([]), freq_items)\n",
    "    # print(freq_items)\n",
    "\n",
    "    start_time = time()\n",
    "    parse_dat = [line.split() for line in open('./machinelearninginaction/Ch12/kosarak.dat').readlines()]\n",
    "    init_set = create_init_set(parse_dat)\n",
    "    my_fp_tree, my_header_tab = create_tree(init_set, 100000)\n",
    "    my_freq_list = []\n",
    "    mine_tree(my_fp_tree, my_header_tab, 100000, set([]), my_freq_list)\n",
    "    end_time = time()\n",
    "    print('被10万或者更多人浏览过的新闻报道或报道集合数:', len(my_freq_list))\n",
    "    print('被10万或者更多人浏览过的新闻报道或报道集合:', my_freq_list)\n",
    "    print('总共执行时间:', end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.矩阵分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch is 0\n",
      "current epoch is 1\n",
      "current epoch is 2\n",
      "current epoch is 3\n",
      "current epoch is 4\n",
      "current epoch is 5\n",
      "current epoch is 6\n",
      "current epoch is 7\n",
      "current epoch is 8\n",
      "current epoch is 9\n",
      "current epoch is 10\n",
      "current epoch is 11\n",
      "current epoch is 12\n",
      "current epoch is 13\n",
      "current epoch is 14\n",
      "current epoch is 15\n",
      "current epoch is 16\n",
      "current epoch is 17\n",
      "current epoch is 18\n",
      "current epoch is 19\n",
      "current epoch is 20\n",
      "current epoch is 21\n",
      "current epoch is 22\n",
      "current epoch is 23\n",
      "current epoch is 24\n",
      "current epoch is 25\n",
      "current epoch is 26\n",
      "current epoch is 27\n",
      "current epoch is 28\n",
      "current epoch is 29\n",
      "current epoch is 30\n",
      "current epoch is 31\n",
      "current epoch is 32\n",
      "current epoch is 33\n",
      "current epoch is 34\n",
      "current epoch is 35\n",
      "current epoch is 36\n",
      "current epoch is 37\n",
      "current epoch is 38\n",
      "current epoch is 39\n",
      "current epoch is 40\n",
      "finish\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.98520814 2.90835283 3.730758   1.01469034]\n",
      " [3.87395941 2.26602062 3.05741769 0.95306741]\n",
      " [1.10612019 0.81618858 5.36105092 4.93829957]\n",
      " [0.99428494 0.7156311  4.34054582 3.94155023]\n",
      " [1.82888212 1.19934321 4.8805316  4.02346927]]\n"
     ]
    }
   ],
   "source": [
    "#funksvd 矩阵分解\n",
    "# coding = utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#========================================================\n",
    "#  准备数据\n",
    "#========================================================\n",
    "\n",
    "def loadDataSet():\n",
    "    R = [\n",
    "        [5,3,0,1],\n",
    "        [4,0,3,1],\n",
    "        [1,1,0,5],\n",
    "        [1,0,0,4],\n",
    "        [0,1,5,4],\n",
    "    ]\n",
    "    return np.array(R)\n",
    "\n",
    "#========================================================\n",
    "#  FunkSVD算法\n",
    "#========================================================\n",
    "\n",
    "def FunkSVD(R, K, epochs = 1000, alpha = 0.1, beta = 0.02):\n",
    "    result=[]\n",
    "\n",
    "    # P、Q矩阵初始化\n",
    "    P = np.random.rand(R.shape[0], 2)\n",
    "    Q = np.random.rand(R.shape[1], 2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"current epoch is {}\".format(epoch))\n",
    "        for i in range(R.shape[0]):\n",
    "            for j in range(R.shape[1]):\n",
    "                if(R[i][j] > 0):\n",
    "                    P[i] = P[i] + alpha * ((R[i][j] - np.dot(P[i],Q[j].T))*Q[j] - beta * P[i])\n",
    "                    Q[j] = Q[j] + alpha * ((R[i][j] - np.dot(P[i],Q[j].T))*P[i] - beta * Q[j])\n",
    "        loss = 0\n",
    "        num = 0\n",
    "        for i in range(R.shape[0]):\n",
    "            for j in range(R.shape[1]):\n",
    "                # 对有评分的项目，计算损失\n",
    "                if(R[i][j] > 0):\n",
    "                    num += 1\n",
    "                    loss += np.power(R[i][j] - np.dot(P[i],Q[j].T), 2)\n",
    "        loss = loss / num\n",
    "        result.append(loss)\n",
    "        if(loss < 0.01):\n",
    "            print(\"finish\")\n",
    "            break\n",
    "    return P, Q, result\n",
    "\n",
    "def plot_train(result):\n",
    "    n = len(result)\n",
    "    x = range(n)\n",
    "    plt.plot(x, result, color='r', linewidth=3)\n",
    "    plt.title(\"Convergence curve\")\n",
    "    plt.xlabel(\"generation\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.show()\n",
    "\n",
    "#========================================================\n",
    "#  主程序\n",
    "#========================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    R = loadDataSet()\n",
    "\n",
    "    pred_P, pred_Q, result = FunkSVD(R, 2)\n",
    "    plot_train(result)\n",
    "    pred_R = np.dot(pred_P, pred_Q.T)\n",
    "    print(pred_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark Mllib 推荐，使用FunkSVD算法\n",
    "# coding = utf-8\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "#========================================================\n",
    "#  数据准备\n",
    "#========================================================\n",
    "\n",
    "sc = SparkContext(\"local\", \"testing\")\n",
    "# 打开用户评分文件，此文件是用户对项目的评分，文件每一行前3项分别是用户id，物品id，评分\n",
    "user_data = sc.textFile(\"D:/movielens/ml-100k/u.data\")\n",
    "# 获取相关数据，即前3列\n",
    "rates = user_data.map(lambda x: x.split(\"\\t\")[0:3])\n",
    "# 将数据封装成 Rating 类\n",
    "from pyspark.mllib.recommendation import Rating\n",
    "rates_data = rates.map(lambda x: Rating(int(x[0]),int(x[1]),int(x[2]))) # 将字符串转为整型\n",
    "\n",
    "#========================================================\n",
    "#  训练模型\n",
    "#========================================================\n",
    "\n",
    "from  pyspark.mllib.recommendation import ALS\n",
    "from pyspark.mllib.recommendation import MatrixFactorizationModel\n",
    "sc.setCheckpointDir('checkpoint/')\n",
    "ALS.checkpointInterval = 2\n",
    "model = ALS.train(ratings=rates_data, rank=20, iterations=5, lambda_=0.02)\n",
    "\n",
    "#========================================================\n",
    "#  预测评分与推荐\n",
    "#========================================================\n",
    "# 预测用户38对物品20的评分\n",
    "print (model.predict(38,20))\n",
    "# 预测用户38最喜欢的10个物品\n",
    "print (model.recommendProducts(38,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
